---
title: "OLS Regression Basics"
author: "Saajan"
date: 2021-04-25
output: html_document
categories: ["Statistics","Machine Learning" ]
tags: ["Machine Learning", "Regression", "Algebra"]
math: TRUE
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
data_url <- "https://raw.githubusercontent.com/saajanrajak/data/main/train_house_price.csv"
raw_data <- read_csv(data_url)
```



# **Introduction**

The Objective of this blog is to understand basic concepts of Regression. We are going to learn today

* **What is Regression?**
* **Different Types of Regression**
* **OLS Regression**
* **Common Terminology in Regression**
* **Regression Calculation in R**
* **Interpretation Regression Results**
* **Different Types of Sum of Squares in Regression**


## **What is Regression?**
Regression is a statistical method used in various field like _Finance, Medical, Manufacturing, Transportation etc_. for predicting future trends by analyzing historical data.  Some examples

* Predicating **Weight of a person** with respect to **Height of a person**
* Predicting **House Price** with respect to  **Size of the House**  and **Number of Bedrooms** variable.
* Predicating Time to reach from Position **A** to **B** from variables **Route, Day,Time of a day, Weather, Holidays, Road category, etc**

Regression Equation
$$\hat Y = \beta_0 +\beta_1X_1 + \beta_2X_2+...\beta_kX_k$$ 


Regression explains the variation in the Dependent variable using variation in Independent variable. In simple words, **Independent variable used to predict Dependent variable**.



## **Different Types of Regression**

Some of the most common types of regression analysis procedure are 

* **Ordinary Linear Regression**: Used when predictor variable and independent variable are linear related. Predicative variable should also be continuous
* **Logistic Regression**: Used when dependent variable have discrete values. Example "True/False", "0/1", "A/B/C" etc
* **Ridge Regression**: Used when there is high correlation between independent variables
* **Lasso Regression**: Used to select subset of predictors for better predication. It shrunk data values towards a central point like mean.
* **Polynomial Regression**: Used when there is non-linear relationship between independent and dependent variables as an $n^{th}$ degree polynomial
* **Bayesian Linear Regression**: Use bayesian theorem to find the value regression coefficients.

 In this blog we will be mostly focusing in **OLS Regression**. 

# **OLS Regression**

While applying Regression, we have to very careful about the data. We should have some logical understanding about the inputs and outputs of the data. There is a common saying in Machine learning world, **Garbage In, Garbage Out**

The goal of regression analysis is to draw a line which minimizes the overall distance of the points from the line. While Calculating we will find that some residuals values are positive and some are negative. Adding all these values will cancel each other, Because of this we use square of residuals for finding our best fit line. In this way they are always positive. This method of finding least value of sum of residual square is called **Ordinary Least Square Regression**

![](\images\Regression Blog\reg_2.gif){ height=250px }

While collecting data for **OLS regression**, We should ensure **Independent Variables must be either Continuous or Categorical/Factor Variables. Dependent variable should be continuous type**. _If Dependent variable is not continuous we will most likely to use different kind of Regression analysis_.


## **Common Terminology in Regression**


* _Variable types_: There are many ways we name a variable in regression.
  + **Input Variables**: Independent Variable, Predictor Variable
  + **Output Variables**: Dependent Variable, Response Variable
  

* _Observed and Predicted Values_
  + **Observed values**: The values of dependent values that we record for during the the study or experiment along with values of independent values. These values are denoted by **Y**.
  + **Predicted values**: The values that model predict for the dependent variable using the independent values. Predicted values also known as Fitted values. These are denoted by $\hat{Y}$.
  + In general terms, **_Observed Values is one that exist in real world, while Predicted Values generated by the model_**.  
* **_Residuals_**: Graphically _Residual are the vertical distance between the observed and fitted values_. 
  + To understand how well the model predicts the value, we need to asses the values of Observed and Predicated values. These differences represent the error in the model.
  + **_No Model is Perfect_**, the observed and predicted values will not always be the same. However models can be good enough to be useful.
  + **Residual = Observed value - Predicted value**. for the $i_{th}$ observation,  $e_i = y_i - \hat{y_i}$
  
  ![](\images\Regression Blog\reg_4.png){ width=20%,height=20% }


## **Regression Calculation in R**

 **Exploratory Data Analysis (EDA)**: Before applying any method for predication, its always better to first understand your data. EDA helps us a lot understanding data. In this blog, we are trying to understand  **OLS Regression**, so we will skipping EDA as of now. In future will create another blog for depth EDA. 
 
 **Standardization of variable**: We should standardize the variable when our regression model contains polynomial terms or interaction terms, these terms produce excessive amount of multicollinearity which leads misleading model results. there are many ways we can perform Standardization/Normalization 
+ Z-Score Scaling
+ Min-Max scaling
+ Standard deviation scaling
  
As we are not planning to use polynomial and interaction term, we will not standardize the variables.  
  
 

We will be using Housing price predication [data](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data "hp data") from Kaggle.We will be considering 
* ***Input Variable***:  **Total Surface Area** (sum of basement, 1st floor, 2nd Floor Area) and **central air** 
* ***Output Variable***:  **Sales price**


```{r echo=FALSE, fig.width=8, fig.height=4}

# creating new variable total surface area and selecting required columns
data <- raw_data %>% 
  mutate(tsf =TotalBsmtSF + `1stFlrSF`+ `2ndFlrSF`) %>% 
  select(TotalBsmtSF, `1stFlrSF`, `2ndFlrSF`, tsf, CentralAir, SalePrice) %>% 
  drop_na()

# Correlation value between total surface area and sale price
cor_value <- round(cor(data$tsf, data$SalePrice),2)


# Plotting histogram for total surface area and understanding the spread
graph_0 <- ggplot(data, aes(tsf))+
  geom_histogram(binwidth = 1000)+
  labs(x = "Total Surface Area", y = "Count", title = "Boxplot Total Surface Area")+
  theme_classic()


## Point chart regression line along Group central air
graph_1 <- ggplot(data, aes(tsf, SalePrice/1000, color = CentralAir))+
           geom_jitter( alpha = 0.3)+
           labs(x= "Total Surface Area", y = "Sale Price (in Thousand $)", 
                title = "Total Surface Area and Sale Price", 
                subtitle = paste("Correlation Value", cor_value, sep =  " " ))+
           geom_smooth(method = "lm", formula = y~x)+
           theme_classic()+
           theme(plot.subtitle = element_text(color = "Blue"))
           

## BOX Plot 
graph_2 <- ggplot(data, aes(CentralAir, SalePrice/1000))+
           geom_boxplot(aes(fill = CentralAir))+
           labs(x = "Central Air", y =" Sale Price (in Thousand $)", title = "BoxPlot Central Air and Sale Price" )+
           theme_classic()+
           theme(legend.position = "none")
           
graph_0
ggpubr::ggarrange(graph_2, graph_1,
                  ncol = 2, nrow = 1)



```

 


* **Function**: There are many functions in R to do Regression, the most common is $lm$ function of base R which we will be using.
    + Syntax of $lm(function, data)$ 
    + formula = $Y \sim f(X_i)$ here 
       + **Y**: Dependent variable 
       + **X**: Independent variable
      


```{r}
## LM model 
model_1 <- lm(formula = "SalePrice ~ tsf + CentralAir", data = data)

model_1
```


## **Interpretation Regression Results**


Basic OLS Regression equation and our Model equation
$$ Y = \beta_0 +\beta_1X_1 + \beta_2X_2 ... \beta_kX_k$$

$$ SalePrice = -43879 + 74 * tsf + 38282 * CentralAir Y$$

We will use $summary$ function to get in depth summary of model.

```{r}
summary(model_1)
```

**Interpretation of Regression terms**

* ***Intercept***: Its just a adjustment constant, there is no physical interpretation

****

* ***Continuous Variable Coefficient***: If all other variable kept constant, there will be $\beta_1$ unit increment in dependent variable for each unit increment of that continuous variable
  + from above equation we can say that, for each one unit increment of total surface area(**tsf**), there will be on average  $74 more **SalePrice**
  + The *signif.Codes* associated to each estimate, three stars (or asterisks) represent a highly significant p-value.
* ***Categorical Variable Coefficient***: If all other variables remains constant, $\beta_2$ is then average difference in $Y$ between the category $X_2=0$ i.e. the reference group and the category for which $X_2 =1$ i.e. the comparison group
  + Here, Central Air is our categorical value having two value (Y and N) for availability and non-availability.  
  + CentralAir**N** is our reference group and CentralAir**Y** is our comparison group
  + So compared to Home with **No Central Air**, we would expect house having **Central Air** will cost more $38282.
     + Central Air No , $Y = \beta_0 + \beta_1X_1 + \epsilon$      
     + Central Air yes, $Y = (\beta_0 + \beta_2)+ \beta_1X_1 + \epsilon$  
     + On average **SalePrice** differ by $\beta_2$ between reference and comparison group.
     
****     
* **Std. Error** It is Residual Standard Error divided by the square root of the sum of the square of that particular x variable.
* **t value** estimate divided by **Std. Error**
* ***Pr(>|t|)***: check for significance, individual variable level
  + Low Value (< 0.05) states variable is significant with at least 95% confidence.

  
****

* ***Residual Standard Error*** Average amount that the response distance will deviate from true regression line.
  + The Degree of Freedom(DOF) is the difference of number of observation and number of coefficients to estimate($\beta_0...\beta_3$). In our case, observation`r nrow(data)`  and estimates 3 therefore DOF is 1457.
  
* ***F-statistic***: Check for significance, overall variables
  + The further the F-statistic is from 1 the better it is
  + It also help in comparing multiple linear models  

* ***Multiple $R^2$***  : Tell us what proportion of the variance is explained by our model. In simple words it tell us how well our model fits the data.
  + Addition of more independent variable increases the Multiple $R^2$ value.
  + If there is only one continuous independent variable and one continuous dependent variable, then multiple $R^2$ is simply the square of correlation of these two variable.
  + Mathematically
 $$R^2 = 1 - \dfrac{SSE}{TSS} = \dfrac{RSS}{TSS} $$
 $$R^2 = 1 - \dfrac{Unexplanied \ Variabilty}{Total \ Variability} = \dfrac{Explanied \ Variabilty}{Total \ Variability}$$
     + SSE: Sum of Square,  $\sum(y - \hat y)^2$
     + TSS: Total Sum of Square, $\sum_{i=1}^{n} (y- \bar y)^2$  
     + RSS: Regression Sum of Square, $\sum (\hat y - \bar y)^2$
        + $y$: observed Value, 
        + $\bar y$ : mean value, 
        + $\hat y$: fitted/modeled value
        

* ***Adjusted $R^2$***: Adjusted $R^2$ normalizes Multiple R-Squared by taking into account how many observation and variables we are using

## **Different Types of Sum of Squares in Regression**

 
> RSS  +  SSE =  TSS 

> Regression  Sum  of   Square   +   Sum    of   Square    of   Error   =   Total   Sum   of  Square

> Explained   variability   +   Unexplained   variability   =   Total   variability

* Comparison of different types of squares
   + **RSS** represents the variability that your model explains. Higher value is usually good.
   + **SSE** represents the variability that your model doesn't explain. Smaller value is usually good
   + **TTS** represents overall variability of dependent variable around its mean

* Mathematical form
  + RSS : Sum of squared distances between the predicted values and mean of the dependent variable $\sum (\hat y - \bar y)^2$.
  + SSE : Sum of squared distances between the observed and predicted values of the dependent variable $\sum(y - \hat y )^2$.
  + TSS : Sum of squared distances between the observed values and mean of the dependent variable $\sum_ (y- \bar y)^2$  

Some important remarks regarding different type of square

* RSS cannot be greater than TSS while SSE cannot be smaller than zero. 
* Some refer RSS as residual sums of squares (which we are calling SSE) rather than regression sums of squares. Be aware of this potentially confusing use of terminology!
* In OLS we should have relatively small and unbiased residuals. OLS regressions are susceptible to outliers. for example consider residuals be (1,3,4) then SSE will be 26, if there is outlier say then residuals (1, 9, 4) then sse will be 98. which will affect the model. Therefore we should be aware of outliers.


Happy Learning :)
 
****
